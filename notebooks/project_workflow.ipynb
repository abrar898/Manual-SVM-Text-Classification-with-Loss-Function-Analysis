{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Cloud-Deployed Text Classifier (SVM)\n",
                "\n",
                "This notebook implements the complete workflow for the SVM Text Classifier project.\n",
                "It includes:\n",
                "1. Data Preparation (IMDb dataset) - **Optimized with Bigrams & No Stopwords**\n",
                "2. Manual SVM Implementation (Hinge, Squared Hinge, Logistic Loss) - **Optimized with Adam Optimizer**\n",
                "3. Scikit-learn Baseline\n",
                "4. Evaluation & Comparison\n",
                "\n",
                "**Target Accuracy**: ~90%+"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies if needed\n",
                "!pip install numpy pandas scikit-learn matplotlib joblib fastapi uvicorn pytest requests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import tarfile\n",
                "import urllib.request\n",
                "import re\n",
                "import time\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import joblib\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.datasets import load_files\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
                "\n",
                "# Set random seed\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "DATA_DIR = \"../data\" if os.path.exists(\"../data\") else \"data\"\n",
                "MODELS_DIR = \"../models\" if os.path.exists(\"../models\") else \"models\"\n",
                "IMDB_URL = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
                "\n",
                "os.makedirs(DATA_DIR, exist_ok=True)\n",
                "os.makedirs(MODELS_DIR, exist_ok=True)\n",
                "\n",
                "def download_and_extract_imdb(data_dir):\n",
                "    imdb_dir = os.path.join(data_dir, \"aclImdb\")\n",
                "    if os.path.exists(imdb_dir):\n",
                "        print(f\"Dataset already exists at {imdb_dir}\")\n",
                "        return imdb_dir\n",
                "\n",
                "    tar_path = os.path.join(data_dir, \"aclImdb_v1.tar.gz\")\n",
                "    if not os.path.exists(tar_path):\n",
                "        print(f\"Downloading IMDb dataset from {IMDB_URL}...\")\n",
                "        urllib.request.urlretrieve(IMDB_URL, tar_path)\n",
                "        print(\"Download complete.\")\n",
                "\n",
                "    print(\"Extracting dataset...\")\n",
                "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
                "        tar.extractall(path=data_dir)\n",
                "    print(\"Extraction complete.\")\n",
                "    return imdb_dir\n",
                "\n",
                "def load_imdb_data(imdb_dir):\n",
                "    print(\"Loading training data...\")\n",
                "    train_data = load_files(os.path.join(imdb_dir, \"train\"), categories=[\"pos\", \"neg\"], encoding=\"utf-8\", shuffle=True, random_state=42)\n",
                "    print(\"Loading test data...\")\n",
                "    test_data = load_files(os.path.join(imdb_dir, \"test\"), categories=[\"pos\", \"neg\"], encoding=\"utf-8\", shuffle=True, random_state=42)\n",
                "    \n",
                "    X = train_data.data + test_data.data\n",
                "    y = np.concatenate([train_data.target, test_data.target])\n",
                "    # Map 0 -> -1 (neg), 1 -> +1 (pos)\n",
                "    y = np.where(y == 0, -1, 1)\n",
                "    return X, y\n",
                "\n",
                "def clean_text(text):\n",
                "    text = re.sub(r'<br />', ' ', text)\n",
                "    return text\n",
                "\n",
                "# Execute Data Loading\n",
                "imdb_dir = download_and_extract_imdb(DATA_DIR)\n",
                "X_text_raw, y = load_imdb_data(imdb_dir)\n",
                "X_text = [clean_text(text) for text in X_text_raw]\n",
                "print(f\"Total samples: {len(X_text)}\")\n",
                "\n",
                "# Vectorize with Bigrams and No Stopwords for higher accuracy\n",
                "print(\"Vectorizing data (TfidfVectorizer, ngram_range=(1, 2), min_df=5)...\")\n",
                "vectorizer = TfidfVectorizer(\n",
                "    ngram_range=(1, 2), \n",
                "    min_df=5,\n",
                "    max_features=50000, \n",
                "    sublinear_tf=True\n",
                ")\n",
                "X = vectorizer.fit_transform(X_text)\n",
                "print(f\"Feature matrix shape: {X.shape}\")\n",
                "\n",
                "# Split 80/20\n",
                "print(\"Splitting data 80/20...\")\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Save Vectorizer\n",
                "joblib.dump(vectorizer, os.path.join(DATA_DIR, \"vectorizer.joblib\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Manual SVM Implementation (Optimized with Adam)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ManualSVM:\n",
                "    def __init__(self, loss='hinge', learning_rate=0.001, lambda_param=0.0001, epochs=10, batch_size=256):\n",
                "        self.loss_type = loss\n",
                "        self.lr = learning_rate\n",
                "        self.lambda_param = lambda_param\n",
                "        self.epochs = epochs\n",
                "        self.batch_size = batch_size\n",
                "        self.w = None\n",
                "        self.b = 0\n",
                "        self.history = {'loss': [], 'accuracy': []}\n",
                "\n",
                "    def _init_weights(self, n_features):\n",
                "        self.w = np.zeros(n_features)\n",
                "        self.b = 0\n",
                "        # Adam parameters\n",
                "        self.m_w = np.zeros(n_features)\n",
                "        self.v_w = np.zeros(n_features)\n",
                "        self.m_b = 0\n",
                "        self.v_b = 0\n",
                "        self.beta1 = 0.9\n",
                "        self.beta2 = 0.999\n",
                "        self.epsilon = 1e-8\n",
                "        self.t = 0\n",
                "\n",
                "    def _compute_loss(self, X, y):\n",
                "        scores = X.dot(self.w) + self.b\n",
                "        if self.loss_type == 'hinge':\n",
                "            losses = np.maximum(0, 1 - y * scores)\n",
                "            data_loss = np.mean(losses)\n",
                "        elif self.loss_type == 'squared_hinge':\n",
                "            losses = np.maximum(0, 1 - y * scores) ** 2\n",
                "            data_loss = np.mean(losses)\n",
                "        elif self.loss_type == 'logistic':\n",
                "            z = -y * scores\n",
                "            data_loss = np.mean(np.logaddexp(0, z))\n",
                "        else:\n",
                "            raise ValueError(f\"Unknown loss: {self.loss_type}\")\n",
                "        reg_loss = self.lambda_param * np.sum(self.w ** 2)\n",
                "        return data_loss + reg_loss\n",
                "\n",
                "    def _compute_gradients(self, X_batch, y_batch):\n",
                "        n_samples = X_batch.shape[0]\n",
                "        scores = X_batch.dot(self.w) + self.b\n",
                "        margins = y_batch * scores\n",
                "        dw = np.zeros_like(self.w)\n",
                "        db = 0\n",
                "        \n",
                "        if self.loss_type == 'hinge':\n",
                "            mask = (1 - margins) > 0\n",
                "            if np.any(mask):\n",
                "                X_active = X_batch[mask]\n",
                "                y_active = y_batch[mask]\n",
                "                dw_data = -X_active.T.dot(y_active) / n_samples\n",
                "                db_data = -np.sum(y_active) / n_samples\n",
                "                dw += dw_data\n",
                "                db += db_data\n",
                "        elif self.loss_type == 'squared_hinge':\n",
                "            mask = (1 - margins) > 0\n",
                "            if np.any(mask):\n",
                "                X_active = X_batch[mask]\n",
                "                y_active = y_batch[mask]\n",
                "                scores_active = scores[mask]\n",
                "                factors = 2 * (1 - y_active * scores_active)\n",
                "                grad_scalars = -factors * y_active\n",
                "                dw_data = X_active.T.dot(grad_scalars) / n_samples\n",
                "                db_data = np.sum(grad_scalars) / n_samples\n",
                "                dw += dw_data\n",
                "                db += db_data\n",
                "        elif self.loss_type == 'logistic':\n",
                "            z = margins\n",
                "            p = np.zeros_like(z)\n",
                "            pos_mask = z >= 0\n",
                "            neg_mask = ~pos_mask\n",
                "            p[pos_mask] = 1 / (1 + np.exp(-z[pos_mask]))\n",
                "            p[neg_mask] = np.exp(z[neg_mask]) / (1 + np.exp(z[neg_mask]))\n",
                "            grad_scalars = (p - 1) * y_batch\n",
                "            dw_data = X_batch.T.dot(grad_scalars) / n_samples\n",
                "            db_data = np.sum(grad_scalars) / n_samples\n",
                "            dw += dw_data\n",
                "            db += db_data\n",
                "            \n",
                "        dw += 2 * self.lambda_param * self.w\n",
                "        return dw, db\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        n_samples, n_features = X.shape\n",
                "        self._init_weights(n_features)\n",
                "        print(f\"Training on {n_samples} samples...\")\n",
                "        for epoch in range(self.epochs):\n",
                "            start_time = time.time()\n",
                "            indices = np.arange(n_samples)\n",
                "            np.random.shuffle(indices)\n",
                "            for start_idx in range(0, n_samples, self.batch_size):\n",
                "                end_idx = min(start_idx + self.batch_size, n_samples)\n",
                "                batch_idx = indices[start_idx:end_idx]\n",
                "                X_batch = X[batch_idx]\n",
                "                y_batch = y[batch_idx]\n",
                "                dw, db = self._compute_gradients(X_batch, y_batch)\n",
                "                \n",
                "                # Adam Update\n",
                "                self.t += 1\n",
                "                self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * dw\n",
                "                self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * db\n",
                "                self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * (dw ** 2)\n",
                "                self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * (db ** 2)\n",
                "                m_w_hat = self.m_w / (1 - self.beta1 ** self.t)\n",
                "                m_b_hat = self.m_b / (1 - self.beta1 ** self.t)\n",
                "                v_w_hat = self.v_w / (1 - self.beta2 ** self.t)\n",
                "                v_b_hat = self.v_b / (1 - self.beta2 ** self.t)\n",
                "                self.w -= self.lr * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
                "                self.b -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
                "                \n",
                "            loss = self._compute_loss(X, y)\n",
                "            acc = self.score(X, y)\n",
                "            self.history['loss'].append(loss)\n",
                "            self.history['accuracy'].append(acc)\n",
                "            print(f\"Epoch {epoch+1}/{self.epochs} - Loss: {loss:.4f} - Acc: {acc:.4f}\")\n",
                "\n",
                "    def predict(self, X):\n",
                "        scores = X.dot(self.w) + self.b\n",
                "        return np.where(scores >= 0, 1, -1)\n",
                "\n",
                "    def score(self, X, y):\n",
                "        preds = self.predict(X)\n",
                "        return np.mean(preds == y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Train Manual Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "losses = ['hinge', 'squared_hinge', 'logistic']\n",
                "manual_models = {}\n",
                "\n",
                "for loss in losses:\n",
                "    print(f\"\\nTraining Manual SVM with {loss} loss...\")\n",
                "    model = ManualSVM(loss=loss, epochs=10, learning_rate=0.001, batch_size=256)\n",
                "    model.fit(X_train, y_train)\n",
                "    manual_models[loss] = model\n",
                "    \n",
                "    # Save model\n",
                "    joblib.dump(model, os.path.join(MODELS_DIR, f\"manual_{loss}.joblib\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Scikit-learn Baseline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training Sklearn LinearSVC...\")\n",
                "sklearn_model = LinearSVC(C=0.1, max_iter=2000, random_state=42)\n",
                "sklearn_model.fit(X_train, y_train)\n",
                "\n",
                "# Save model\n",
                "joblib.dump(sklearn_model, os.path.join(MODELS_DIR, \"sklearn_linear_svc.joblib\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation & Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate(model, X, y, name):\n",
                "    preds = model.predict(X)\n",
                "    return {\n",
                "        \"Model\": name,\n",
                "        \"Accuracy\": accuracy_score(y, preds),\n",
                "        \"Precision\": precision_score(y, preds, pos_label=1),\n",
                "        \"Recall\": recall_score(y, preds, pos_label=1),\n",
                "        \"F1\": f1_score(y, preds, pos_label=1)\n",
                "    }\n",
                "\n",
                "results = []\n",
                "\n",
                "# Evaluate Manual Models\n",
                "for loss, model in manual_models.items():\n",
                "    results.append(evaluate(model, X_test, y_test, f\"Manual {loss.replace('_', ' ').title()}\"))\n",
                "\n",
                "# Evaluate Sklearn Model\n",
                "results.append(evaluate(sklearn_model, X_test, y_test, \"Sklearn LinearSVC\"))\n",
                "\n",
                "df_results = pd.DataFrame(results)\n",
                "print(df_results)\n",
                "\n",
                "# Save results\n",
                "df_results.to_csv(os.path.join(\"../report\" if os.path.exists(\"../report\") else \"report\", \"comparison_table.csv\"), index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Plot Losses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 6))\n",
                "for loss, model in manual_models.items():\n",
                "    plt.plot(model.history['loss'], label=f\"Manual {loss}\")\n",
                "\n",
                "plt.xlabel(\"Epochs\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.title(\"Training Loss Comparison\")\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}